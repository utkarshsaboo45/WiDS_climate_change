{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# WiDS","metadata":{}},{"cell_type":"markdown","source":"## Data Loading","metadata":{}},{"cell_type":"markdown","source":"__Importing the libraries__","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import (\n    OneHotEncoder,\n    StandardScaler,\n    FunctionTransformer,\n    PolynomialFeatures\n)\n\nfrom sklearn.impute import SimpleImputer\n# from sklearn.impute import IterativeImputer\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\nfrom sklearn.feature_selection import RFECV\n\nfrom sklearn.model_selection import (\n    cross_val_score,\n    cross_validate,\n    train_test_split,\n    RandomizedSearchCV\n)\n\nfrom sklearn.linear_model import Ridge, Lasso\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom catboost import CatBoostRegressor\nfrom lightgbm.sklearn import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.ensemble import StackingRegressor\n\nfrom sklearn.metrics import make_scorer\n# from sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve","metadata":{"execution":{"iopub.status.busy":"2022-02-23T10:59:57.030974Z","iopub.execute_input":"2022-02-23T10:59:57.031721Z","iopub.status.idle":"2022-02-23T10:59:58.491150Z","shell.execute_reply.started":"2022-02-23T10:59:57.031619Z","shell.execute_reply":"2022-02-23T10:59:58.490413Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom torchvision import transforms, datasets, utils\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset, random_split, Dataset\nfrom torch import Tensor\nfrom torch.nn import Linear\nfrom torch.nn import Sigmoid\nfrom torch.nn import Module\nfrom torch.optim import SGD\nfrom torch.nn import MSELoss\nimport torch.utils.data as data\nfrom torch import nn","metadata":{"execution":{"iopub.status.busy":"2022-02-23T10:59:59.695934Z","iopub.execute_input":"2022-02-23T10:59:59.696493Z","iopub.status.idle":"2022-02-23T11:00:00.237190Z","shell.execute_reply.started":"2022-02-23T10:59:59.696453Z","shell.execute_reply":"2022-02-23T11:00:00.236423Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_path = \"../input/wids2022/train.csv\"\ntest_path = \"../input/wids2022/test.csv\"\n# train_path = \"data/train.csv\"\n# test_path = \"data/test.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-02-23T11:00:02.434653Z","iopub.execute_input":"2022-02-23T11:00:02.435273Z","iopub.status.idle":"2022-02-23T11:00:02.441764Z","shell.execute_reply.started":"2022-02-23T11:00:02.435229Z","shell.execute_reply":"2022-02-23T11:00:02.440919Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Dataprep(Dataset):\n\n    def __init__(self, train_path, test_path, size=0.1, is_train=True):\n        df = pd.read_csv(train_path)\n        test_df = pd.read_csv(test_path)\n\n        TARGET_COLUMN = \"site_eui\"\n\n        self.is_train = is_train\n\n        train_df, val_df = train_test_split(\n            df,\n            test_size=0.1,\n            random_state=123\n        )\n\n        self.X_train, self.y_train = train_df.drop(columns=[TARGET_COLUMN]), train_df[TARGET_COLUMN]\n        self.X_val, self.y_val = val_df.drop(columns=[TARGET_COLUMN]), val_df[TARGET_COLUMN]\n        self.X_test = test_df\n\n    def engineer_features(self):\n        # Find Standard Deviation of min, max and avg temp among months\n        min_temps = [\n            \"january_min_temp\",\n            \"february_min_temp\",\n            \"march_min_temp\",\n            \"april_min_temp\",\n            \"may_min_temp\",\n            \"june_min_temp\",\n            \"july_min_temp\",\n            \"august_min_temp\",\n            \"september_min_temp\",\n            \"october_min_temp\",\n            \"november_min_temp\",\n            \"december_min_temp\"\n        ]\n\n        max_temps = [\n            \"january_max_temp\",\n            \"february_max_temp\",\n            \"march_max_temp\",\n            \"april_max_temp\",\n            \"may_max_temp\",\n            \"june_max_temp\",\n            \"july_max_temp\",\n            \"august_max_temp\",\n            \"september_max_temp\",\n            \"october_max_temp\",\n            \"november_max_temp\",\n            \"december_max_temp\"\n        ]\n\n        avg_temps = [\n            \"january_avg_temp\",\n            \"february_avg_temp\",\n            \"march_avg_temp\",\n            \"april_avg_temp\",\n            \"may_avg_temp\",\n            \"june_avg_temp\",\n            \"july_avg_temp\",\n            \"august_avg_temp\",\n            \"september_avg_temp\",\n            \"october_avg_temp\",\n            \"november_avg_temp\",\n            \"december_avg_temp\",\n        ]\n\n        self.numeric_features = [\n            \"floor_area\",\n            \"year_built\",\n            \"energy_star_rating\",\n            \"ELEVATION\",\n            \"cooling_degree_days\",\n            \"heating_degree_days\",\n            \"precipitation_inches\",\n            \"snowfall_inches\",\n            \"snowdepth_inches\",\n            \"avg_temp\",\n            \"days_below_30F\",\n            \"days_below_20F\",\n            \"days_below_10F\",\n            \"days_below_0F\",\n            \"days_above_80F\",\n            \"days_above_90F\",\n            \"days_above_100F\",\n            \"days_above_110F\",\n            \"max_wind_speed\",\n            \"days_with_fog\",\n            \"building_age\",\n            \"min_temp_std\",\n            \"max_temp_std\",\n            \"avg_temp_std\",\n            \"0-10\",\n            \"10-20\",\n            \"20-30\",\n            \"30-80\",\n            \"80-90\",\n            \"90-100\",\n            \"100-110\"\n        ] + min_temps + max_temps + avg_temps\n\n        self.categorical_features = [\n            \"Year_Factor\",\n            \"State_Factor\",\n            \"building_class\",\n            \"facility_type\",\n            \"direction_max_wind_speed\",\n            \"direction_peak_wind_speed\"\n        ]\n\n        self.drop_columns = [\n            \"id\"\n        ]\n\n        for X in [self.X_train, self.X_val, self.X_test]:\n            X[\"building_age\"] = 2022 - X[\"year_built\"]\n\n        for X in [self.X_train, self.X_val, self.X_test]:\n            X[\"min_temp_std\"] = X[min_temps].T.std()\n            X[\"max_temp_std\"] = X[max_temps].T.std()\n            X[\"avg_temp_std\"] = X[avg_temps].T.std()\n\n        days_above_below = [\n            \"days_below_30F\",\n            \"days_below_20F\",\n            \"days_below_10F\",\n            \"days_below_0F\",\n            \"days_above_80F\",\n            \"days_above_90F\",\n            \"days_above_100F\",\n            \"days_above_110F\",\n        ]\n\n        for X in [self.X_train, self.X_val, self.X_test]:\n            X[\"0-10\"] = X[\"days_below_10F\"] - X[\"days_below_0F\"]\n            X[\"10-20\"] = X[\"days_below_20F\"] - X[\"days_below_10F\"]\n            X[\"20-30\"] = X[\"days_below_30F\"] - X[\"days_below_20F\"]\n            X[\"80-90\"] = X[\"days_above_80F\"] - X[\"days_above_90F\"]\n            X[\"90-100\"] = X[\"days_above_90F\"] - X[\"days_above_100F\"]\n            X[\"100-110\"] = X[\"days_above_100F\"] - X[\"days_above_110F\"]\n            X[\"30-80\"] = (366 - X[days_above_below].sum(axis=1)).clip(lower=0)\n\n#         residential = [\n#             \"Multifamily_Uncategorized\",\n#             \"2to4_Unit_Building\",\n#             \"5plus_Unit_Building\",\n#             \"Religious_worship\",\n#             \"Parking_Garage\",\n#             \"Mixed_Use_Predominantly_Residential\"\n#         ]\n\n#         industrial = [\n#             \"Warehouse_Nonrefrigerated\",\n#             \"Warehouse_Distribution_or_Shipping_center\",\n#             \"Warehouse_Selfstorage\",\n#             \"Industrial\",\n#             \"Warehouse_Uncategorized\",\n#             \"Warehouse_Refrigerated\",\n#             \"Laboratory\",\n#             \"Data_Center\"\n#         ]\n\n#         commercial = list(\n#             set(self.X_train[\"facility_type\"].value_counts().index) -\n#             set(residential) - set(industrial)\n#         )\n\n#         temp = pd.concat([self.X_train[\"facility_type\"], feature_set.y_train], axis=1)\n#         convert_dic = dict(temp[[\"facility_type\", \"site_eui\"]].groupby(\"facility_type\")[\"site_eui\"].mean().sort_values())\n\n#         for X in [self.X_train, self.X_val, self.X_test]:\n#             for facility_type in [\n#                 \"Food\",\n#                 \"Education\",\n#                 \"Health_Care\",\n#                 \"Public_Assembly\",\n#                 \"Public_Safety\",\n#                 \"Lodging\",\n#                 \"Warehouse\",\n#                 \"Office\",\n#                 \"Service\",\n#                 \"Retail\"\n#             ]:\n#                 X[\"facility_type\"] = [\n#                     facility_type if f_type.lower().startswith(facility_type.lower())\n#                     else f_type\n#                     for f_type in X[\"facility_type\"]\n#                 ]\n# #             X.replace({\"facility_type\": convert_dic}, inplace=True)\n# #         #     X[\"facility_type\"] = [\n# #         #             \"commercial\" if f_type in commercial\n# #         #             else \"residential\" if f_type in residential\n# #         #             else \"industrial\"\n# #         #         for f_type in X[\"facility_type\"]]\n\n\n    def preprocess(self):\n        pipe_numeric_feats = make_pipeline(\n           SimpleImputer(strategy=\"mean\"),\n           StandardScaler()\n        )\n        pipe_cat_feats = make_pipeline(\n            SimpleImputer(strategy=\"most_frequent\"),\n            OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n        )\n        self.column_transformer = make_column_transformer(\n            (pipe_numeric_feats, self.numeric_features),\n            (pipe_cat_feats, self.categorical_features)\n        )\n\n        self.X_train_raw = self.X_train\n        self.X_val_raw = self.X_val\n        self.X_test_raw = self.X_test\n        self.X_train = self.column_transformer.fit_transform(self.X_train)\n        self.X_val = self.column_transformer.transform(self.X_val)\n        self.X_test = self.column_transformer.transform(self.X_test)\n\n        self.X_train_tensor = torch.tensor(self.X_train, dtype=torch.float32)\n        self.y_train_tensor = torch.tensor(self.y_train.values, dtype=torch.float32)\n        self.X_val_tensor = torch.tensor(self.X_val, dtype=torch.float32)\n        self.y_val_tensor = torch.tensor(self.y_val.values, dtype=torch.float32)\n        self.X_test_tensor = torch.tensor(self.X_test, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T11:00:04.449696Z","iopub.execute_input":"2022-02-23T11:00:04.450003Z","iopub.status.idle":"2022-02-23T11:00:04.479014Z","shell.execute_reply.started":"2022-02-23T11:00:04.449966Z","shell.execute_reply":"2022-02-23T11:00:04.478161Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"feature_set = Dataprep(train_path, test_path)\nfeature_set.engineer_features()\nfeature_set.preprocess()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T11:00:05.421479Z","iopub.execute_input":"2022-02-23T11:00:05.422122Z","iopub.status.idle":"2022-02-23T11:00:12.598441Z","shell.execute_reply.started":"2022-02-23T11:00:05.422080Z","shell.execute_reply":"2022-02-23T11:00:12.597677Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"############################################ Temp zone #################################################\n\n# Apply imputation in all columns\n\n# Tried: X_train[\"facility_type\"].value_counts() -> Categorize in 3-4 categories\n# energy level / floor area group by mean and variance\n\n# Time series seasonal component in the monthly data -> Try feeding to RNNs\n# Take three month windows to calculate average -> Repeat it to narrow 12 features down to 1 or try SVD\n\n# use site_eui to order facility_type","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################################################################################################","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mse(predictions, targets):\n    return np.mean((predictions - targets) ** 2)\n        \ndef rmse(predictions, targets):\n    return np.sqrt(((predictions - targets) ** 2).mean())\n\ndef mape(true, pred): \n    return 100.0 * np.mean(np.abs((pred - true) / true))\n\ndef r2score_torch(predictions, target):\n    target_mean = torch.mean(target)\n    ss_tot = torch.sum((target - target_mean) ** 2)\n    ss_res = torch.sum((target - predictions) ** 2)\n    r2 = 1 - ss_res / ss_tot\n    return r2\n\ndef mse_torch(predictions, targets):\n    return torch.mean((predictions - targets) ** 2)\n        \ndef rmse_torch(predictions, targets):\n    return torch.sqrt(((predictions - targets) ** 2).mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-23T11:00:12.600149Z","iopub.execute_input":"2022-02-23T11:00:12.600475Z","iopub.status.idle":"2022-02-23T11:00:12.610890Z","shell.execute_reply.started":"2022-02-23T11:00:12.600437Z","shell.execute_reply":"2022-02-23T11:00:12.610088Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Sklearn Models","metadata":{}},{"cell_type":"code","source":"for dt in [\n    feature_set.X_train,\n    feature_set.X_val,\n    feature_set.y_train,\n    feature_set.y_val\n]:\n    print(dt.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T11:00:12.611982Z","iopub.execute_input":"2022-02-23T11:00:12.612231Z","iopub.status.idle":"2022-02-23T11:00:12.622385Z","shell.execute_reply.started":"2022-02-23T11:00:12.612195Z","shell.execute_reply":"2022-02-23T11:00:12.621493Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def cross_val_scores(model, X_train, y_train, X_val, y_val, return_train_score=False):\n\n    model.fit(X_train, y_train)\n    y_val_pred = model.predict(X_val)\n\n    score_dict = {\n        \"r2_val\": model.score(X_val, y_val),\n        \"mse_val\": mse(y_val, y_val_pred),\n        \"rmse_val\": rmse(y_val, y_val_pred),\n        \"mape_val\": mape(y_val, y_val_pred)\n    }\n\n    if return_train_score:\n        y_train_pred = model.predict(X_train)\n\n        score_dict[\"r2_train\"] = model.score(X_train, y_train)\n        score_dict[\"mse_train\"] = mse(y_train, y_train_pred)\n        score_dict[\"rmse_train\"] = rmse(y_train, y_train_pred)\n        score_dict[\"mape_train\"] = mape(y_train, y_train_pred)\n\n    scores_result = pd.Series(score_dict)\n\n    return model, scores_result","metadata":{"execution":{"iopub.status.busy":"2022-02-23T11:00:14.321032Z","iopub.execute_input":"2022-02-23T11:00:14.321576Z","iopub.status.idle":"2022-02-23T11:00:14.329065Z","shell.execute_reply.started":"2022-02-23T11:00:14.321533Z","shell.execute_reply":"2022-02-23T11:00:14.327845Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"pipe_ridge = make_pipeline(feature_set.column_transformer, Ridge(max_iter=10000))\npipe_lasso = make_pipeline(feature_set.column_transformer, Lasso())\npipe_rf = make_pipeline(feature_set.column_transformer, RandomForestRegressor())\npipe_xgb = make_pipeline(feature_set.column_transformer, XGBRegressor(verbosity=0))#, eta=0.01, max_depth=7, n_estimators=1000))\npipe_lgbm = make_pipeline(feature_set.column_transformer, LGBMRegressor())\npipe_catboost = make_pipeline(feature_set.column_transformer, CatBoostRegressor(verbose=False))","metadata":{"execution":{"iopub.status.busy":"2022-02-23T11:00:23.377161Z","iopub.execute_input":"2022-02-23T11:00:23.377936Z","iopub.status.idle":"2022-02-23T11:00:23.383750Z","shell.execute_reply.started":"2022-02-23T11:00:23.377891Z","shell.execute_reply":"2022-02-23T11:00:23.383074Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"results = {}\n\ndef train(models, results):\n    for name, model in models.items():\n        print(f\"Start {name}!\")\n        start_time = time.time()\n        _, results[name] = cross_val_scores(\n            model,\n            feature_set.X_train_raw,\n            feature_set.y_train,\n            feature_set.X_val_raw,\n            feature_set.y_val,\n            return_train_score=True\n        )\n\n        print(f\"Done {name} in {round(time.time() - start_time)} secs!\")\n\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-02-23T11:02:13.144148Z","iopub.execute_input":"2022-02-23T11:02:13.144434Z","iopub.status.idle":"2022-02-23T11:02:13.150808Z","shell.execute_reply.started":"2022-02-23T11:02:13.144401Z","shell.execute_reply":"2022-02-23T11:02:13.150088Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"models = {\n    \"Ridge\": pipe_ridge,\n    \"Lasso\": pipe_lasso,\n    \"Random Forest\": pipe_rf,\n    \"XGB\": pipe_xgb,\n    \"LGBM\": pipe_lgbm,\n    \"Cat Boost\": pipe_catboost,\n}","metadata":{"execution":{"iopub.status.busy":"2022-02-23T11:03:25.922003Z","iopub.execute_input":"2022-02-23T11:03:25.922720Z","iopub.status.idle":"2022-02-23T11:03:25.927141Z","shell.execute_reply.started":"2022-02-23T11:03:25.922675Z","shell.execute_reply":"2022-02-23T11:03:25.926117Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"results = train(models, results)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T11:03:41.627335Z","iopub.execute_input":"2022-02-23T11:03:41.627603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfecv = RFECV(Ridge(), min_features_to_select=40, n_jobs=-1)\n\npipe_<>_rfecv = make_pipeline(\n    feature_set.column_transformer, rfecv, <>Regressor(random_state=42, verbose=False)\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poly_feats = PolynomialFeatures(degree=2)\n\npipe_poly_ridge = make_pipeline(\n    column_transformer, poly_feats, Ridge()\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_rfe_poly_ridge = {\n#     \"<>_rfecv\": pipe_<>_rfecv,\n#     \"Poly Ridge\": pipe_poly_ridge\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = train(models_rfe_poly_ridge, results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mape_scorer = make_scorer(mape, greater_is_better=False)\n\nscoring_metrics = {\n    \"neg RMSE\": \"neg_root_mean_squared_error\",\n    \"r2\": \"r2\",\n    \"mape\": mape_scorer\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperparam Tune Random Forest \n\nparams_rf = {\n    'randomforestregressor__n_estimators': [10, 100, 500, 1000],\n    'randomforestregressor__max_depth': [5, 10, 12],\n    'randomforestregressor__max_features': ['auto', 'sqrt']\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_search_rf = RandomizedSearchCV(\n    pipe_rf,\n    params_rf,\n    n_jobs=-1,\n    n_iter=20,\n    return_train_score=True,\n    scoring=scoring_metrics,\n    refit=\"r2\"\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# random_search_rf.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_lgbm = {\n    'lgbmregressor__n_estimators': [10, 100, 1000],\n    'lgbmregressor__max_depth': [5, 10, 15]\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_search_lgbm = RandomizedSearchCV(\n    pipe_lgbm,\n    params_lgbm,\n    n_jobs=-1,\n    n_iter=20,\n    return_train_score=True,\n    scoring=scoring_metrics,\n    refit=\"r2\"\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# random_search_lgbm.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HyperparamTune XGBoost\nparams_xgb = {\n    'xgbregressor__n_estimators': [10, 100, 1000],\n    'xgbregressor__max_depth': [3, 5, 7, 12],\n    'xgbregressor__eta': [0.01, 0.03, 0.01, 0.3]\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_search_xgb = RandomizedSearchCV(\n    pipe_xgb,\n    params_xgb,\n    n_jobs=-1,\n    n_iter=20,\n    return_train_score=True,\n    scoring=scoring_metrics,\n    refit=\"r2\"\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# random_search_xgb.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pd.DataFrame(random_search.cv_results_)[[\n#         \"mean_fit_time\",\n#         \"mean_score_time\",\n#         \"param_ridge__alpha\",\n#         \"mean_train_neg RMSE\",\n#         \"std_train_neg RMSE\",\n#         \"mean_test_mape\",\n#         \"mean_train_mape\",\n#         \"mean_test_r2\",\n#         \"mean_train_r2\"\n#     ]\n# ].sort_values(by='mean_test_r2', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Stacking","metadata":{}},{"cell_type":"code","source":"models_selected = {\n    \"Ridge\": pipe_ridge,\n    \"Lasso\": pipe_lasso,\n    \"XGB\": pipe_xgb,\n    \"LGBM\": pipe_lgbm,\n#     \"<>_rfecv\": pipe_<>_rfecv,\n#     \"Poly Ridge\": pipe_poly_ridge,\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacking_model = StackingRegressor(list(models_selected.items()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name = \"Stacking\"\n\nprint(f\"Start {name}!\")\nstart_time = time.time()\n\n_, results[name] = cross_val_scores(\n    stacking_model,\n    feature_set.X_train_raw,\n    feature_set.y_train,\n    feature_set.X_val_raw,\n    feature_set.y_val,\n    return_train_score=True\n)\n\nprint(f\"Done {name} in {round(time.time() - start_time)} secs!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FCNN","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def linear_block(input_size, output_size):\n    return nn.Sequential(\n        nn.Linear(input_size, output_size),\n        nn.ReLU(),\n        nn.Dropout(0.2)\n    )\n\nclass Extractlastcell(nn.Module):\n    def forward(self, x):\n        out, _ = x\n        return out[-1]\n\n\nclass EnergyRegressor(nn.Module):\n    def __init__(self, input_size):\n        super(EnergyRegressor, self).__init__()\n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=2 * input_size, num_layers=2)\n        self.tanh = nn.Tanh()\n        self.layers = nn.Sequential(\n            linear_block(2 * input_size, 3 * input_size),\n            linear_block(3 * input_size, 5 * input_size),\n            linear_block(5 * input_size, 10 * input_size),\n            linear_block(10 * input_size, 7 * input_size),\n            linear_block(7 * input_size, 5 * input_size),\n            linear_block(5 * input_size, 3 * input_size),\n            linear_block(3 * input_size, input_size),\n            nn.Linear(input_size, 1000),\n            nn.Linear(1000, 700),\n            nn.Linear(700, 400), \n            nn.Linear(400, 256),\n            nn.Linear(256, 128),\n            nn.Linear(128, 64),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, X):\n        X = X.to(device)\n        # X (sequence length, batch size, input size)\n        X = X.reshape(1, X.shape[0], X.shape[1])\n        X, _ = self.lstm(X)\n        X = X[-1]\n        X = self.tanh(X)\n        X = self.layers(X)\n        return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainloader = DataLoader(TensorDataset(feature_set.X_train_tensor, feature_set.y_train_tensor), batch_size=32, shuffle=True)\nvalidloader = DataLoader(TensorDataset(feature_set.X_val_tensor, feature_set.y_val_tensor), batch_size=32, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = EnergyRegressor(feature_set.X_train_tensor.shape[1])\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trainer(model, criterion, optimizer, trainloader, validloader, epochs):\n    train_mse = 0\n    train_rmse = 0\n    train_r2 = 0\n    val_mse = 0\n    val_rmse = 0\n    val_r2 = 0\n    \n    for epoch in range(epochs):\n        train_batch_mse = []\n        train_batch_rmse = []\n        train_batch_r2 = []\n        val_batch_mse = []\n        val_batch_rmse = []\n        val_batch_r2 = []\n\n        model.train(True)\n\n        for X, y in trainloader:\n            X = X.to(device)\n            y = y.to(device)\n            y_hat = model(X).flatten()\n            \n            optimizer.zero_grad()\n            loss = criterion(y_hat, y)\n            loss = loss.to(device)\n            loss.backward()\n            optimizer.step()\n            mse_train = mse_torch(y_hat, y)\n            rmse_train = rmse_torch(y_hat, y)\n            r2_train = r2score_torch(y_hat, y)\n            train_batch_mse.append(mse_train)\n            train_batch_rmse.append(rmse_train)\n            train_batch_r2.append(r2_train)\n        \n        train_mse = torch.sum(torch.Tensor(train_batch_mse)) / len(trainloader)\n        train_rmse = torch.sum(torch.Tensor(train_batch_rmse)) / len(trainloader)\n        train_r2 = torch.sum(torch.Tensor(train_batch_r2)) / len(trainloader)\n\n        model.eval()\n\n        with torch.no_grad():\n            for X_valid, y_valid in validloader:\n                X_valid = X_valid.to(device)\n                y_valid = y_valid.to(device).flatten()\n                y_hat_val = model(X_valid)\n                mse_val = mse_torch(y_hat_val, y_valid)\n                rmse_val = rmse_torch(y_hat_val, y_valid)\n                r2_val = r2score_torch(y_hat_val, y_valid)\n                val_batch_mse.append(mse_val)\n                val_batch_rmse.append(rmse_val)\n                val_batch_r2.append(r2_val)\n            val_mse = torch.sum(torch.Tensor(val_batch_mse)) / len(validloader)\n            val_rmse = torch.sum(torch.Tensor(val_batch_rmse)) / len(validloader)\n            val_r2 = torch.sum(torch.Tensor(val_batch_r2)) / len(validloader) \n\n        print(f\"Epoch {epoch + 1}:\\tTrain:\\tMSE: {round(train_mse.item(), 4)}. RMSE: {round(train_rmse.item(), 4)}, R2: {round(train_r2.item(), 4)}.\")\n        print(f\"\\t\\tVal:\\tMSE: {round(val_mse.item(), 4)}, RMSE: {round(val_rmse.item(), 4)}, R2: {round(val_r2.item(), 4)}.\")\n        print(\"-\" * 80)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\ncriterion = criterion.to(device)\ntrained_model = trainer(model, criterion, optimizer, trainloader, validloader, epochs=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, X):\n    return model(X.type(torch.float32))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = predict(trained_model, feature_set.X_test_tensor)\npredictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_dict = {\"id\": feature_set.X_test_raw[\"id\"],\n               \"site_eui\": predictions.cpu().detach().numpy().flatten()}\npd.DataFrame(results_dict).set_index(\"id\").to_csv(\"submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}